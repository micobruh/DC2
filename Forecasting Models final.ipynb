{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":953,"status":"ok","timestamp":1647526885009,"user":{"displayName":"Stefan Robu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18158011255744518542"},"user_tz":-60},"id":"lpjZoiBzQPou","outputId":"bf928704-f373-4b74-ebce-4ac4af4a8309"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from sklearn.model_selection import train_test_split,RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor\n","sns.set()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#reading the dataset\n","df2=pd.read_csv(\"all_variables.csv\")\n","df2_data=df2[['Year','LSOA_code','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December','Type_II_crime_amount']]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#removing the COVID data\n","df2_data=df2_data[df2_data['Year']<2020]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#extracting the years to loop over them later\n","years=df2_data['Year'].unique()"]},{"cell_type":"markdown","metadata":{},"source":["## Random Forest implementation-final\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#Creating the tables for the LSOA codes which we will focus on\n","set1=df2_data[df2_data['LSOA_code']=='E01000005']\n","set2=df2_data[df2_data['LSOA_code']=='E01030759']\n","set3=df2_data[df2_data['LSOA_code']=='E01020795']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["#removing the LSOA code feature in each dataset\n","set1=set1.drop(['LSOA_code'],axis=1)\n","set2=set2.drop(['LSOA_code'],axis=1)\n","set3=set3.drop(['LSOA_code'],axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#Numeric array for the months of a year\n","months=[1,2,3,4,5,6,7,8,9,10,11,12]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#hyperparameter tuning\n","\n","#choosing the parameters\n","n_estimators=[int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n","max_features=['auto','sqrt']\n","max_depth=[int(x) for x in np.linspace(10,110, num=11)]\n","max_depth.append(None)\n","min_samples_split=[2,5,10]\n","min_samples_leaf=[1,2,4]\n","bootstrap=[True,False]\n","\n","#creating a dictionary to use in the hyperparameter tuning\n","random_grid={'n_estimators':n_estimators,\n","                'max_features':max_features,\n","                'max_depth':max_depth,\n","                'min_samples_split':min_samples_split,\n","                'min_samples_leaf':min_samples_leaf,\n","                'bootstrap':bootstrap}\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"]},{"data":{"text/plain":["RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n","                   n_jobs=-1,\n","                   param_distributions={'bootstrap': [True, False],\n","                                        'max_depth': [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, 110,\n","                                                      None],\n","                                        'max_features': ['auto', 'sqrt'],\n","                                        'min_samples_leaf': [1, 2, 4],\n","                                        'min_samples_split': [2, 5, 10],\n","                                        'n_estimators': [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   random_state=35, verbose=2)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#tuning the hyperparameters for the type 1 crimes for area 1\n","\n","#selecting the data for the type 1 cries for the area 1\n","type1_crimes=set1[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#getting the training and the test years for the tuning\n","train_year_type1=type1_crimes[type1_crimes['Year']==2012]\n","test_year_type1=type1_crimes[type1_crimes['Year']==2013]\n","#dropping the year feature from the data\n","train_year_type1=train_year_type1.drop(['Year'],axis=1)\n","test_year_type1=test_year_type1.drop(['Year'],axis=1)\n","#creating a new feature for the months\n","train_year_type1['Month']=months\n","test_year_type1['Month']=months\n","#removing the one hot encoding for the months\n","train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","#creating the X and the Y for both training and test sets\n","X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","y_type1_train=train_year_type1['Type_I_crime_amount']\n","X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","y_type1_test=test_year_type1['Type_I_crime_amount']\n","#creating the random forest regressor and the method through which we will tune the hyperparameters\n","rf=RandomForestRegressor()\n","rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n","rf_random.fit(X_type1_train,y_type1_train)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"]},{"data":{"text/plain":["RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n","                   n_jobs=-1,\n","                   param_distributions={'bootstrap': [True, False],\n","                                        'max_depth': [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, 110,\n","                                                      None],\n","                                        'max_features': ['auto', 'sqrt'],\n","                                        'min_samples_leaf': [1, 2, 4],\n","                                        'min_samples_split': [2, 5, 10],\n","                                        'n_estimators': [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   random_state=35, verbose=2)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#hyperparameter tuning for the type 2 crimes for area 1\n","type2_crimes=set1[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#getting the training and the test years for the tuning\n","train_year_type2=type2_crimes[type2_crimes['Year']==2012]\n","test_year_type2=type2_crimes[type1_crimes['Year']==2013]\n","#dropping the year feature from the data\n","train_year_type2=train_year_type2.drop(['Year'],axis=1)\n","test_year_type2=test_year_type2.drop(['Year'],axis=1)\n","#creating a new feature for the months\n","train_year_type2['Month']=months\n","test_year_type2['Month']=months\n","#removing the one hot encoding for the months\n","train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","#creating the X and the Y for both training and test sets\n","X_type1_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","y_type1_train=train_year_type2['Type_II_crime_amount']\n","X_type1_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","y_type1_test=test_year_type2['Type_II_crime_amount']\n","#creating the random forest regressor and the method through which we will tune the hyperparameters\n","rf=RandomForestRegressor()\n","rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n","rf_random.fit(X_type1_train,y_type1_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#this prints the best parameters for the random forest regressor (must be run after each dataset was tuned)\n","rf_random.best_params_"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#area 1\n","#Separating the crimes data in this are for each crime type\n","type1_crimes=set1[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","type2_crimes=set1[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#arrays where we will keep the results of our evaluations\n","r2_type1=[]\n","r2_type2=[]\n","rmse_type1=[]\n","rmse_type2=[]\n","mae_type1=[]\n","mae_type2=[]\n","year_sofar=[]\n","#Looping through the years where we will retrain the model based on the data of said year\n","for i in range(1,len(years)):\n","    #adding in the year to the array of years which have been used so far\n","    year_sofar.append(years[i])\n","    #training the data on year X-1 if we want to predict year X (e.g if we want to predict the crime numbers for year 2013, we will train on 2012)\n","    train_year_type1=type1_crimes[type1_crimes['Year']==years[i-1]]\n","    train_year_type2=type2_crimes[type2_crimes['Year']==years[i-1]]\n","    test_year_type1=type1_crimes[type1_crimes['Year']==years[i]]\n","    test_year_type2=type2_crimes[type2_crimes['Year']==years[i]]\n","    #dropping the year feature from the data\n","    train_year_type1=train_year_type1.drop(['Year'],axis=1)\n","    train_year_type2=train_year_type2.drop(['Year'],axis=1)\n","    test_year_type1=test_year_type1.drop(['Year'],axis=1)\n","    test_year_type2=test_year_type2.drop(['Year'],axis=1)\n","    #creating a new feature for the months\n","    train_year_type1['Month']=months\n","    train_year_type2['Month']=months\n","    test_year_type1['Month']=months\n","    test_year_type2['Month']=months\n","    #removing the one hot encoding for the months\n","    train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    #creating the X and the Y for both training and test sets for both types of crimes\n","    X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","    X_type2_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","    y_type1_train=train_year_type1['Type_I_crime_amount']\n","    y_type2_train=train_year_type2['Type_II_crime_amount']\n","    X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","    X_type2_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","    y_type1_test=test_year_type1['Type_I_crime_amount']\n","    y_type2_test=test_year_type2['Type_II_crime_amount']\n","    #Creating two random forest regressors for each type of crime\n","    rf_type1=RandomForestRegressor(n_estimators=400,min_samples_split=2,min_samples_leaf=4,max_features='auto',max_depth=20,bootstrap=True,random_state=0)\n","    rf_type2=RandomForestRegressor(n_estimators=400,min_samples_split=10,min_samples_leaf=2,max_features='sqrt',max_depth=100,bootstrap=True,random_state=0)\n","    rf_type1.fit(X_type1_train,y_type1_train)\n","    rf_type2.fit(X_type2_train,y_type2_train)\n","    #predicting the crime numbers for the test set for both types of crimes\n","    y_pred_type1=rf_type1.predict(X_type1_test)\n","    y_pred_type2=rf_type2.predict(X_type2_test)\n","    #calculating the r2 score for both types of crimes\n","    r2_type1.append(r2_score(y_type1_test,y_pred_type1))\n","    r2_type2.append(r2_score(y_type2_test,y_pred_type2))\n","    #calculating the rmse score for both types of crimes\n","    rmse_type1.append(np.sqrt(mean_squared_error(y_type1_test,y_pred_type1)))\n","    rmse_type2.append(np.sqrt(mean_squared_error(y_type2_test,y_pred_type2)))\n","    #calculating the mae score for both types of crimes\n","    mae_type2.append(mean_absolute_error(y_type2_test,y_pred_type2,multioutput='raw_values'))\n","    mae_type1.append(mean_absolute_error(y_type1_test,y_pred_type1,multioutput='raw_values'))\n","#combining the years array with the score array for each score for each crime type\n","rmse_area1_type1=list(zip(year_sofar,rmse_type1))\n","rmse_area1_type2=list(zip(year_sofar,rmse_type2))\n","mae_area1_type1=list(zip(year_sofar,mae_type1))\n","mae_area1_type2=list(zip(year_sofar,mae_type2))\n","r2_area1_type2=list(zip(year_sofar,r2_type2))\n","r2_area1_type1=list(zip(year_sofar,r2_type1))\n","#creating dataframes for the scores for each crime type\n","rmse_area1_type1_table=pd.DataFrame(rmse_area1_type1,columns=['Year','RMSE_Type_I'])\n","rmse_area1_type2_table=pd.DataFrame(rmse_area1_type2,columns=['Year','RMSE_Type_II'])\n","mae_area1_type1_table=pd.DataFrame(mae_area1_type1,columns=['Year','MAE_Type_I'])\n","mae_area1_type2_table=pd.DataFrame(mae_area1_type2,columns=['Year','MAE_Type_II'])\n","r2_area1_type1_table=pd.DataFrame(r2_area1_type1,columns=['Year','R2_Type_I'])\n","r2_area1_type2_table=pd.DataFrame(r2_area1_type2,columns=['Year','R2_Type_II'])\n","#outputting the data to csv files\n","rmse_area1_type1_table.to_csv('rmse_area1_type1.csv')\n","rmse_area1_type2_table.to_csv('rmse_area1_type2.csv')\n","mae_area1_type1_table.to_csv('mae_area1_type1.csv')\n","mae_area1_type2_table.to_csv('mae_area1_type2.csv')\n","r2_area1_type1_table.to_csv('r2_area1_type1.csv')\n","r2_area1_type2_table.to_csv('r2_area1_type2.csv')\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"]},{"data":{"text/plain":["RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n","                   n_jobs=-1,\n","                   param_distributions={'bootstrap': [True, False],\n","                                        'max_depth': [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, 110,\n","                                                      None],\n","                                        'max_features': ['auto', 'sqrt'],\n","                                        'min_samples_leaf': [1, 2, 4],\n","                                        'min_samples_split': [2, 5, 10],\n","                                        'n_estimators': [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   random_state=35, verbose=2)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#hyperparameter tuning for the second area\n","\n","#creating the arrays for the first crime type\n","type1_crimes=set2[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#creating the arrays for the training and test year \n","train_year_type1=type1_crimes[type1_crimes['Year']==2012]\n","test_year_type1=type1_crimes[type1_crimes['Year']==2013]\n","#dropping the year column from both the training and test sets\n","train_year_type1=train_year_type1.drop(['Year'],axis=1)\n","test_year_type1=test_year_type1.drop(['Year'],axis=1)\n","#adding the numerical value for months in each set\n","train_year_type1['Month']=months\n","test_year_type1['Month']=months\n","#dropping the one hot encoded month columns\n","train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","#creating the X and Y arrays for the training and test sets\n","X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","y_type1_train=train_year_type1['Type_I_crime_amount']\n","X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","y_type1_test=test_year_type1['Type_I_crime_amount']\n","#creating the random forest regressor and the hyperparameter which will be used for training\n","rf=RandomForestRegressor()\n","rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n","rf_random.fit(X_type1_train,y_type1_train)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"]},{"data":{"text/plain":["RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n","                   n_jobs=-1,\n","                   param_distributions={'bootstrap': [True, False],\n","                                        'max_depth': [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, 110,\n","                                                      None],\n","                                        'max_features': ['auto', 'sqrt'],\n","                                        'min_samples_leaf': [1, 2, 4],\n","                                        'min_samples_split': [2, 5, 10],\n","                                        'n_estimators': [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   random_state=35, verbose=2)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#creating the array for the second crime type\n","type2_crimes=set2[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#creating the arrays for the training and test year\n","train_year_type2=type2_crimes[type2_crimes['Year']==2012]\n","test_year_type2=type2_crimes[type1_crimes['Year']==2013]\n","#dropping the year column from both the training and test sets\n","train_year_type2=train_year_type2.drop(['Year'],axis=1)\n","test_year_type2=test_year_type2.drop(['Year'],axis=1)\n","#adding the numerical value for months in each set\n","train_year_type2['Month']=months\n","test_year_type2['Month']=months\n","#dropping the one hot encoded month columns\n","train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","#creating the X and Y arrays for the training and test sets\n","X_type1_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","y_type1_train=train_year_type2['Type_II_crime_amount']\n","X_type1_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","y_type1_test=test_year_type2['Type_II_crime_amount']\n","#creating the random forest regressor and the hyperparameter which will be used for training\n","rf=RandomForestRegressor()\n","rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n","rf_random.fit(X_type1_train,y_type1_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Outputting the best parameters after tuning\n","rf_random.best_params_"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#creating the arrays for each crime type\n","type1_crimes=set2[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","type2_crimes=set2[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#creating the arrays to store our evaluation results\n","r2_type1=[]\n","r2_type2=[]\n","rmse_type1=[]\n","rmse_type2=[]\n","mae_type1=[]\n","mae_type2=[]\n","year_sofar=[]\n","#loop to iterate through the years\n","for i in range(1,len(years)):\n","    #adding the already evaluated years to the year_sofar array\n","    year_sofar.append(years[i])\n","    #creating the arrays for the training and test year\n","    train_year_type1=type1_crimes[type1_crimes['Year']==years[i-1]]\n","    train_year_type2=type2_crimes[type2_crimes['Year']==years[i-1]]\n","    test_year_type1=type1_crimes[type1_crimes['Year']==years[i]]\n","    test_year_type2=type2_crimes[type2_crimes['Year']==years[i]]\n","    #dropping the year column from both the training and test sets\n","    train_year_type1=train_year_type1.drop(['Year'],axis=1)\n","    train_year_type2=train_year_type2.drop(['Year'],axis=1)\n","    test_year_type1=test_year_type1.drop(['Year'],axis=1)\n","    test_year_type2=test_year_type2.drop(['Year'],axis=1)\n","    #adding the numerical value for months in each set\n","    train_year_type1['Month']=months\n","    train_year_type2['Month']=months\n","    test_year_type1['Month']=months\n","    test_year_type2['Month']=months\n","    #dropping the one hot encoded month columns\n","    train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    #creating the X and Y arrays for the training and test sets\n","    X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","    X_type2_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","    y_type1_train=train_year_type1['Type_I_crime_amount']\n","    y_type2_train=train_year_type2['Type_II_crime_amount']\n","    X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","    X_type2_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","    y_type1_test=test_year_type1['Type_I_crime_amount']\n","    y_type2_test=test_year_type2['Type_II_crime_amount']\n","    #creating two separate random forest regressor for each crime type\n","    rf_type1=RandomForestRegressor(n_estimators=400,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',max_depth=18,bootstrap=False,random_state=0)\n","    rf_type2=RandomForestRegressor(n_estimators=200,min_samples_split=10,min_samples_leaf=2,max_features='auto',max_depth=60,bootstrap=True,random_state=0)\n","    rf_type1.fit(X_type1_train,y_type1_train)\n","    rf_type2.fit(X_type2_train,y_type2_train)\n","    #predicting the values for the test set\n","    y_pred_type1=rf_type1.predict(X_type1_test)\n","    y_pred_type2=rf_type2.predict(X_type2_test)\n","    #calculating the r2 score for each crime type\n","    r2_type1.append(r2_score(y_type1_test,y_pred_type1))\n","    r2_type2.append(r2_score(y_type2_test,y_pred_type2))\n","    #calculating the rmse for each crime type\n","    rmse_type1.append(np.sqrt(mean_squared_error(y_type1_test,y_pred_type1)))\n","    rmse_type2.append(np.sqrt(mean_squared_error(y_type2_test,y_pred_type2)))\n","    #calculating the mae for each crime type\n","    mae_type1.append(mean_absolute_error(y_type1_test,y_pred_type1,multioutput='raw_values'))\n","    mae_type2.append(mean_absolute_error(y_type2_test,y_pred_type2,multioutput='raw_values'))\n","\n","#combining the array for the navigated years with the evaluation results for each metric    \n","rmse_area2_type1=list(zip(year_sofar,rmse_type1))\n","rmse_area2_type2=list(zip(year_sofar,rmse_type2))\n","mae_area2_type1=list(zip(year_sofar,mae_type1))\n","mae_area2_type2=list(zip(year_sofar,mae_type2))\n","r2_area2_type2=list(zip(year_sofar,r2_type2))\n","r2_area2_type1=list(zip(year_sofar,r2_type1))\n","#creating dataframes for the evaluation results\n","rmse_area2_type1_table=pd.DataFrame(rmse_area2_type1,columns=['Year','RMSE_Type_I'])\n","rmse_area2_type2_table=pd.DataFrame(rmse_area2_type2,columns=['Year','RMSE_Type_II'])\n","mae_area2_type1_table=pd.DataFrame(mae_area2_type1,columns=['Year','MAE_Type_I'])\n","mae_area2_type2_table=pd.DataFrame(mae_area2_type2,columns=['Year','MAE_Type_II'])\n","r2_area2_type1_table=pd.DataFrame(r2_area2_type1,columns=['Year','R2_Type_I'])\n","r2_area2_type2_table=pd.DataFrame(r2_area2_type2,columns=['Year','R2_Type_II'])\n","#outputting the data to csv files\n","rmse_area2_type1_table.to_csv('rmse_area2_type1.csv')\n","rmse_area2_type2_table.to_csv('rmse_area2_type2.csv')\n","mae_area2_type1_table.to_csv('mae_area2_type1.csv')\n","mae_area2_type2_table.to_csv('mae_area2_type2.csv')\n","r2_area2_type1_table.to_csv('r2_area2_type1.csv')\n","r2_area2_type2_table.to_csv('r2_area2_type2.csv')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"]},{"data":{"text/plain":["RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n","                   n_jobs=-1,\n","                   param_distributions={'bootstrap': [True, False],\n","                                        'max_depth': [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, 110,\n","                                                      None],\n","                                        'max_features': ['auto', 'sqrt'],\n","                                        'min_samples_leaf': [1, 2, 4],\n","                                        'min_samples_split': [2, 5, 10],\n","                                        'n_estimators': [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   random_state=35, verbose=2)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["#creating the dataset which will be used for tuning\n","type1_crimes=set3[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#Separating the data into test and training years\n","train_year_type1=type1_crimes[type1_crimes['Year']==2012]\n","test_year_type1=type1_crimes[type1_crimes['Year']==2013]\n","#dropping the year column\n","train_year_type1=train_year_type1.drop(['Year'],axis=1)\n","test_year_type1=test_year_type1.drop(['Year'],axis=1)\n","#adding the numerical values for months\n","train_year_type1['Month']=months\n","test_year_type1['Month']=months\n","#dropping the one-hot encoded columns\n","train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","#creating the X and Y variables for both training and test data\n","X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","y_type1_train=train_year_type1['Type_I_crime_amount']\n","X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","y_type1_test=test_year_type1['Type_I_crime_amount']\n","#creating the random forest regressor and the hyperparameter tuning method\n","rf=RandomForestRegressor()\n","rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n","rf_random.fit(X_type1_train,y_type1_train)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"]},{"data":{"text/plain":["RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n","                   n_jobs=-1,\n","                   param_distributions={'bootstrap': [True, False],\n","                                        'max_depth': [10, 20, 30, 40, 50, 60,\n","                                                      70, 80, 90, 100, 110,\n","                                                      None],\n","                                        'max_features': ['auto', 'sqrt'],\n","                                        'min_samples_leaf': [1, 2, 4],\n","                                        'min_samples_split': [2, 5, 10],\n","                                        'n_estimators': [200, 400, 600, 800,\n","                                                         1000, 1200, 1400, 1600,\n","                                                         1800, 2000]},\n","                   random_state=35, verbose=2)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["#creating the dataframe for the tuning of the hyperparameters\n","type2_crimes=set3[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#Separating the data into test and training years\n","train_year_type2=type2_crimes[type2_crimes['Year']==2012]\n","test_year_type2=type2_crimes[type1_crimes['Year']==2013]\n","#dropping the year column\n","train_year_type2=train_year_type2.drop(['Year'],axis=1)\n","test_year_type2=test_year_type2.drop(['Year'],axis=1)\n","#adding the numerical values for months\n","train_year_type2['Month']=months\n","test_year_type2['Month']=months\n","#dropping the one-hot encoded columns\n","train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","#creating the X and Y variables for both training and test data\n","X_type1_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","y_type1_train=train_year_type2['Type_II_crime_amount']\n","X_type1_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","y_type1_test=test_year_type2['Type_II_crime_amount']\n","#creating the random forest regressor and the hyperparameter tuning method\n","rf=RandomForestRegressor()\n","rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n","rf_random.fit(X_type1_train,y_type1_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#printing the best parameters after tuning (must be run after each tune for each crime type)\n","rf_random.best_params_"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#creating the dataframe for each crime type\n","type1_crimes=set3[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","type2_crimes=set3[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n","#creating arrays where we will store the results\n","year_sofar=[]\n","r2_type1=[]\n","r2_type2=[]\n","rmse_type1=[]\n","rmse_type2=[]\n","mae_type1=[]\n","mae_type2=[]\n","year_sofar=[]\n","#looping through the years\n","for i in range(1,len(years)):\n","    #addng the evaluated years to the array\n","    year_sofar.append(years[i])\n","    #creating the data for the training and test years\n","    train_year_type1=type1_crimes[type1_crimes['Year']==years[i-1]]\n","    train_year_type2=type2_crimes[type2_crimes['Year']==years[i-1]]\n","    test_year_type1=type1_crimes[type1_crimes['Year']==years[i]]\n","    test_year_type2=type2_crimes[type2_crimes['Year']==years[i]]\n","    #dropping the year column\n","    train_year_type1=train_year_type1.drop(['Year'],axis=1)\n","    train_year_type2=train_year_type2.drop(['Year'],axis=1)\n","    test_year_type1=test_year_type1.drop(['Year'],axis=1)\n","    test_year_type2=test_year_type2.drop(['Year'],axis=1)\n","    #adding the numerical values for months\n","    train_year_type1['Month']=months\n","    train_year_type2['Month']=months\n","    test_year_type1['Month']=months\n","    test_year_type2['Month']=months\n","    #dropping the one-hot encoded columns\n","    train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n","    #creating the X and Y variables for both training and test data\n","    X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","    X_type2_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","    y_type1_train=train_year_type1['Type_I_crime_amount']\n","    y_type2_train=train_year_type2['Type_II_crime_amount']\n","    X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n","    X_type2_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n","    y_type1_test=test_year_type1['Type_I_crime_amount']\n","    y_type2_test=test_year_type2['Type_II_crime_amount']\n","    #creating the random forest regressors for each crime tyoe\n","    rf_type1=RandomForestRegressor(n_estimators=600,min_samples_split=10,min_samples_leaf=2,max_features='sqrt',max_depth=None,bootstrap=True,random_state=0)\n","    rf_type2=RandomForestRegressor(n_estimators=800,min_samples_split=5,min_samples_leaf=4,max_features='auto',max_depth=110,bootstrap=True,random_state=0)\n","    rf_type1.fit(X_type1_train,y_type1_train)\n","    rf_type2.fit(X_type2_train,y_type2_train)\n","    #predicting the results for each crime type\n","    y_pred_type1=rf_type1.predict(X_type1_test)\n","    y_pred_type2=rf_type2.predict(X_type2_test)\n","    #calculating the r2 score for each crime type\n","    r2_type1.append(r2_score(y_type1_test,y_pred_type1))\n","    r2_type2.append(r2_score(y_type2_test,y_pred_type2))\n","    #calculating the rmse for each crime type\n","    rmse_type1.append(np.sqrt(mean_squared_error(y_type1_test,y_pred_type1)))\n","    rmse_type2.append(np.sqrt(mean_squared_error(y_type2_test,y_pred_type2)))\n","    #calculating the mae for each crime type\n","    mae_type1.append(mean_absolute_error(y_type1_test,y_pred_type1,multioutput='raw_values'))\n","    mae_type2.append(mean_absolute_error(y_type2_test,y_pred_type2,multioutput='raw_values'))\n","#combining the data for the years for each evaluation metric\n","rmse_area3_type1=list(zip(year_sofar,rmse_type1))\n","rmse_area3_type2=list(zip(year_sofar,rmse_type2))\n","mae_area3_type1=list(zip(year_sofar,mae_type1))\n","mae_area3_type2=list(zip(year_sofar,mae_type2))\n","r2_area3_type2=list(zip(year_sofar,r2_type2))\n","r2_area3_type1=list(zip(year_sofar,r2_type1))\n","#creating the dataframes for the results\n","rmse_area3_type1_table=pd.DataFrame(rmse_area3_type1,columns=['Year','RMSE_Type_I'])\n","rmse_area3_type2_table=pd.DataFrame(rmse_area3_type2,columns=['Year','RMSE_Type_II'])\n","mae_area3_type1_table=pd.DataFrame(mae_area3_type1,columns=['Year','MAE_Type_I'])\n","mae_area3_type2_table=pd.DataFrame(mae_area3_type2,columns=['Year','MAE_Type_II'])\n","r2_area3_type1_table=pd.DataFrame(r2_area3_type1,columns=['Year','R2_Type_I'])\n","r2_area3_type2_table=pd.DataFrame(r2_area3_type2,columns=['Year','R2_Type_II'])\n","#outputting the dataframes to csv files\n","rmse_area3_type1_table.to_csv('rmse_area3_type1.csv')\n","rmse_area3_type2_table.to_csv('rmse_area3_type2.csv')\n","mae_area3_type1_table.to_csv('mae_area3_type1.csv')\n","mae_area3_type2_table.to_csv('mae_area3_type2.csv')\n","r2_area3_type1_table.to_csv('r2_area3_type1.csv')\n","r2_area3_type2_table.to_csv('r2_area3_type2.csv')"]}],"metadata":{"colab":{"name":"Forecasting Models.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
