{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpjZoiBzQPou"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error,median_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WieCIL9YbsRG"
      },
      "outputs": [],
      "source": [
        "#reading the dataset\n",
        "df2=pd.read_csv(\"all_variables.csv\")\n",
        "df2_data=df2[['Year','LSOA_code','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December','Type_II_crime_amount']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjrY-axXbsRH"
      },
      "outputs": [],
      "source": [
        "#removing the COVID data\n",
        "df2_data=df2_data[df2_data['Year']<2020]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQrVTaCQbsRI"
      },
      "outputs": [],
      "source": [
        "#extracting the years to loop over them later\n",
        "years=df2_data['Year'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dopql-bUbsRI"
      },
      "source": [
        "## Random Forest implementation-final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuL7vmtBbsRR"
      },
      "outputs": [],
      "source": [
        "#Creating the tables for the LSOA codes which we will focus on\n",
        "set1=df2_data[df2_data['LSOA_code']=='E01000005']\n",
        "set2=df2_data[df2_data['LSOA_code']=='E01030759']\n",
        "set3=df2_data[df2_data['LSOA_code']=='E01020795']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vapEeOWqbsRS"
      },
      "outputs": [],
      "source": [
        "#removing the LSOA code feature in each dataset\n",
        "set1=set1.drop(['LSOA_code'],axis=1)\n",
        "set2=set2.drop(['LSOA_code'],axis=1)\n",
        "set3=set3.drop(['LSOA_code'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_ma2s1_bsRT"
      },
      "outputs": [],
      "source": [
        "#Numeric array for the months of a year\n",
        "months=[1,2,3,4,5,6,7,8,9,10,11,12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3YT4-Z3bsRU"
      },
      "outputs": [],
      "source": [
        "#hyperparameter tuning\n",
        "\n",
        "#choosing the parameters\n",
        "n_estimators=[int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
        "max_features=['auto','sqrt']\n",
        "max_depth=[int(x) for x in np.linspace(10,110, num=11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split=[2,5,10]\n",
        "min_samples_leaf=[1,2,4]\n",
        "bootstrap=[True,False]\n",
        "\n",
        "#creating a dictionary to use in the hyperparameter tuning\n",
        "random_grid={'n_estimators':n_estimators,\n",
        "                'max_features':max_features,\n",
        "                'max_depth':max_depth,\n",
        "                'min_samples_split':min_samples_split,\n",
        "                'min_samples_leaf':min_samples_leaf,\n",
        "                'bootstrap':bootstrap}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDrjmPSYbsRU",
        "outputId": "fe00c1ab-0b70-41ca-edc0-71e147e8ab8d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'random_grid' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32116/270029844.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#creating the random forest regressor and the method through which we will tune the hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mrf_random\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mrf_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_type1_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_type1_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'random_grid' is not defined"
          ]
        }
      ],
      "source": [
        "#tuning the hyperparameters for the type 1 crimes for area 1\n",
        "\n",
        "#selecting the data for the type 1 cries for the area 1\n",
        "type1_crimes=set1[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#getting the training and the test years for the tuning\n",
        "train_year_type1=type1_crimes[type1_crimes['Year']==2012]\n",
        "test_year_type1=type1_crimes[type1_crimes['Year']==2013]\n",
        "#dropping the year feature from the data\n",
        "train_year_type1=train_year_type1.drop(['Year'],axis=1)\n",
        "test_year_type1=test_year_type1.drop(['Year'],axis=1)\n",
        "#creating a new feature for the months\n",
        "train_year_type1['Month']=months\n",
        "test_year_type1['Month']=months\n",
        "#removing the one hot encoding for the months\n",
        "train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "#creating the X and the Y for both training and test sets\n",
        "X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "y_type1_train=train_year_type1['Type_I_crime_amount']\n",
        "X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "y_type1_test=test_year_type1['Type_I_crime_amount']\n",
        "#creating the random forest regressor and the method through which we will tune the hyperparameters\n",
        "rf=RandomForestRegressor()\n",
        "rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n",
        "rf_random.fit(X_type1_train,y_type1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGpaQSCmbsRW",
        "outputId": "92a0bb0e-8ed1-40e3-d7b1-75065f6a9c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
              "                   n_jobs=-1,\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   random_state=35, verbose=2)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#hyperparameter tuning for the type 2 crimes for area 1\n",
        "type2_crimes=set1[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#getting the training and the test years for the tuning\n",
        "train_year_type2=type2_crimes[type2_crimes['Year']==2012]\n",
        "test_year_type2=type2_crimes[type1_crimes['Year']==2013]\n",
        "#dropping the year feature from the data\n",
        "train_year_type2=train_year_type2.drop(['Year'],axis=1)\n",
        "test_year_type2=test_year_type2.drop(['Year'],axis=1)\n",
        "#creating a new feature for the months\n",
        "train_year_type2['Month']=months\n",
        "test_year_type2['Month']=months\n",
        "#removing the one hot encoding for the months\n",
        "train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "#creating the X and the Y for both training and test sets\n",
        "X_type1_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "y_type1_train=train_year_type2['Type_II_crime_amount']\n",
        "X_type1_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "y_type1_test=test_year_type2['Type_II_crime_amount']\n",
        "#creating the random forest regressor and the method through which we will tune the hyperparameters\n",
        "rf=RandomForestRegressor()\n",
        "rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n",
        "rf_random.fit(X_type1_train,y_type1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dULXCIybsRW"
      },
      "outputs": [],
      "source": [
        "#this prints the best parameters for the random forest regressor (must be run after each dataset was tuned)\n",
        "rf_random.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqKwPn0RbsRX"
      },
      "outputs": [],
      "source": [
        "#area 1\n",
        "#Separating the crimes data in this are for each crime type\n",
        "type1_crimes=set1[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "type2_crimes=set1[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#arrays where we will keep the results of our evaluations\n",
        "r2_type1=[]\n",
        "r2_type2=[]\n",
        "rmse_type1=[]\n",
        "rmse_type2=[]\n",
        "mae_type1=[]\n",
        "mae_type2=[]\n",
        "medae_type1=[]\n",
        "medae_type2=[]\n",
        "year_sofar=[]\n",
        "#Looping through the years where we will retrain the model based on the data of said year\n",
        "for i in range(1,len(years)):\n",
        "    #adding in the year to the array of years which have been used so far\n",
        "    year_sofar.append(years[i])\n",
        "    #training the data on year X-1 if we want to predict year X (e.g if we want to predict the crime numbers for year 2013, we will train on 2012)\n",
        "    train_year_type1=type1_crimes[type1_crimes['Year']==years[i-1]]\n",
        "    train_year_type2=type2_crimes[type2_crimes['Year']==years[i-1]]\n",
        "    test_year_type1=type1_crimes[type1_crimes['Year']==years[i]]\n",
        "    test_year_type2=type2_crimes[type2_crimes['Year']==years[i]]\n",
        "    #dropping the year feature from the data\n",
        "    train_year_type1=train_year_type1.drop(['Year'],axis=1)\n",
        "    train_year_type2=train_year_type2.drop(['Year'],axis=1)\n",
        "    test_year_type1=test_year_type1.drop(['Year'],axis=1)\n",
        "    test_year_type2=test_year_type2.drop(['Year'],axis=1)\n",
        "    #creating a new feature for the months\n",
        "    train_year_type1['Month']=months\n",
        "    train_year_type2['Month']=months\n",
        "    test_year_type1['Month']=months\n",
        "    test_year_type2['Month']=months\n",
        "    #removing the one hot encoding for the months\n",
        "    train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    #creating the X and the Y for both training and test sets for both types of crimes\n",
        "    X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "    X_type2_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "    y_type1_train=train_year_type1['Type_I_crime_amount']\n",
        "    y_type2_train=train_year_type2['Type_II_crime_amount']\n",
        "    X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "    X_type2_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "    y_type1_test=test_year_type1['Type_I_crime_amount']\n",
        "    y_type2_test=test_year_type2['Type_II_crime_amount']\n",
        "    #Creating two random forest regressors for each type of crime\n",
        "    rf_type1=RandomForestRegressor(n_estimators=400,min_samples_split=2,min_samples_leaf=4,max_features='auto',max_depth=20,bootstrap=True,random_state=0)\n",
        "    rf_type2=RandomForestRegressor(n_estimators=400,min_samples_split=10,min_samples_leaf=2,max_features='sqrt',max_depth=100,bootstrap=True,random_state=0)\n",
        "    rf_type1.fit(X_type1_train,y_type1_train)\n",
        "    rf_type2.fit(X_type2_train,y_type2_train)\n",
        "    #predicting the crime numbers for the test set for both types of crimes\n",
        "    y_pred_type1=rf_type1.predict(X_type1_test)\n",
        "    y_pred_type2=rf_type2.predict(X_type2_test)\n",
        "    #calculating the r2 score for both types of crimes\n",
        "    r2_type1.append(r2_score(y_type1_test,y_pred_type1))\n",
        "    r2_type2.append(r2_score(y_type2_test,y_pred_type2))\n",
        "    #calculating the rmse score for both types of crimes\n",
        "    rmse_type1.append(np.sqrt(mean_squared_error(y_type1_test,y_pred_type1)))\n",
        "    rmse_type2.append(np.sqrt(mean_squared_error(y_type2_test,y_pred_type2)))\n",
        "    #calculating the mae score for both types of crimes\n",
        "    mae_type2.append(mean_absolute_error(y_type2_test,y_pred_type2,multioutput='raw_values'))\n",
        "    mae_type1.append(mean_absolute_error(y_type1_test,y_pred_type1,multioutput='raw_values'))\n",
        "    #calculating the medae score for both types of crimes\n",
        "    medae_type1.append(median_absolute_error(y_type1_test,y_pred_type1))\n",
        "    medae_type2.append(median_absolute_error(y_type2_test,y_pred_type2))\n",
        "#combining the years array with the score array for each score for each crime type\n",
        "rmse_area1_type1=list(zip(year_sofar,rmse_type1))\n",
        "rmse_area1_type2=list(zip(year_sofar,rmse_type2))\n",
        "mae_area1_type1=list(zip(year_sofar,mae_type1))\n",
        "mae_area1_type2=list(zip(year_sofar,mae_type2))\n",
        "r2_area1_type2=list(zip(year_sofar,r2_type2))\n",
        "r2_area1_type1=list(zip(year_sofar,r2_type1))\n",
        "medae_area1_type1=list(zip(year_sofar,medae_type1))\n",
        "medae_area1_type2=list(zip(year_sofar,medae_type2))\n",
        "#creating dataframes for the scores for each crime type\n",
        "rmse_area1_type1_table=pd.DataFrame(rmse_area1_type1,columns=['Year','RMSE_Type_I'])\n",
        "rmse_area1_type2_table=pd.DataFrame(rmse_area1_type2,columns=['Year','RMSE_Type_II'])\n",
        "mae_area1_type1_table=pd.DataFrame(mae_area1_type1,columns=['Year','MAE_Type_I'])\n",
        "mae_area1_type2_table=pd.DataFrame(mae_area1_type2,columns=['Year','MAE_Type_II'])\n",
        "r2_area1_type1_table=pd.DataFrame(r2_area1_type1,columns=['Year','R2_Type_I'])\n",
        "r2_area1_type2_table=pd.DataFrame(r2_area1_type2,columns=['Year','R2_Type_II'])\n",
        "medae_area1_type1_table=pd.DataFrame(medae_area1_type1,columns=['Year','MEDAE_Type_I'])\n",
        "medae_area1_type2_table=pd.DataFrame(medae_area1_type2,columns=['Year','MEDAE_Type_II'])\n",
        "#outputting the data to csv files\n",
        "rmse_area1_type1_table.to_csv('rmse_area1_type1.csv')\n",
        "rmse_area1_type2_table.to_csv('rmse_area1_type2.csv')\n",
        "mae_area1_type1_table.to_csv('mae_area1_type1.csv')\n",
        "mae_area1_type2_table.to_csv('mae_area1_type2.csv')\n",
        "r2_area1_type1_table.to_csv('r2_area1_type1.csv')\n",
        "r2_area1_type2_table.to_csv('r2_area1_type2.csv')\n",
        "medae_area1_type1_table.to_csv('medae_area1_type1.csv')\n",
        "medae_area1_type2_table.to_csv('medae_area1_type2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElWx5UssbsRY",
        "outputId": "55a4c5db-91a6-40e1-9e41-828fc98d1b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
              "                   n_jobs=-1,\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   random_state=35, verbose=2)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#hyperparameter tuning for the second area\n",
        "\n",
        "#creating the arrays for the first crime type\n",
        "type1_crimes=set2[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#creating the arrays for the training and test year \n",
        "train_year_type1=type1_crimes[type1_crimes['Year']==2012]\n",
        "test_year_type1=type1_crimes[type1_crimes['Year']==2013]\n",
        "#dropping the year column from both the training and test sets\n",
        "train_year_type1=train_year_type1.drop(['Year'],axis=1)\n",
        "test_year_type1=test_year_type1.drop(['Year'],axis=1)\n",
        "#adding the numerical value for months in each set\n",
        "train_year_type1['Month']=months\n",
        "test_year_type1['Month']=months\n",
        "#dropping the one hot encoded month columns\n",
        "train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "#creating the X and Y arrays for the training and test sets\n",
        "X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "y_type1_train=train_year_type1['Type_I_crime_amount']\n",
        "X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "y_type1_test=test_year_type1['Type_I_crime_amount']\n",
        "#creating the random forest regressor and the hyperparameter which will be used for training\n",
        "rf=RandomForestRegressor()\n",
        "rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n",
        "rf_random.fit(X_type1_train,y_type1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBYzHB-dbsRY",
        "outputId": "988e2698-eb41-41b2-8bfc-c7b22f0a4431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
              "                   n_jobs=-1,\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   random_state=35, verbose=2)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#creating the array for the second crime type\n",
        "type2_crimes=set2[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#creating the arrays for the training and test year\n",
        "train_year_type2=type2_crimes[type2_crimes['Year']==2012]\n",
        "test_year_type2=type2_crimes[type1_crimes['Year']==2013]\n",
        "#dropping the year column from both the training and test sets\n",
        "train_year_type2=train_year_type2.drop(['Year'],axis=1)\n",
        "test_year_type2=test_year_type2.drop(['Year'],axis=1)\n",
        "#adding the numerical value for months in each set\n",
        "train_year_type2['Month']=months\n",
        "test_year_type2['Month']=months\n",
        "#dropping the one hot encoded month columns\n",
        "train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "#creating the X and Y arrays for the training and test sets\n",
        "X_type1_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "y_type1_train=train_year_type2['Type_II_crime_amount']\n",
        "X_type1_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "y_type1_test=test_year_type2['Type_II_crime_amount']\n",
        "#creating the random forest regressor and the hyperparameter which will be used for training\n",
        "rf=RandomForestRegressor()\n",
        "rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n",
        "rf_random.fit(X_type1_train,y_type1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB08mSj9bsRY"
      },
      "outputs": [],
      "source": [
        "#Outputting the best parameters after tuning\n",
        "rf_random.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWXoJ8WtbsRZ"
      },
      "outputs": [],
      "source": [
        "#creating the arrays for each crime type\n",
        "type1_crimes=set2[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "type2_crimes=set2[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#creating the arrays to store our evaluation results\n",
        "r2_type1=[]\n",
        "r2_type2=[]\n",
        "rmse_type1=[]\n",
        "rmse_type2=[]\n",
        "mae_type1=[]\n",
        "mae_type2=[]\n",
        "year_sofar=[]\n",
        "medae_type1=[]\n",
        "medae_type2=[]\n",
        "#loop to iterate through the years\n",
        "for i in range(1,len(years)):\n",
        "    #adding the already evaluated years to the year_sofar array\n",
        "    year_sofar.append(years[i])\n",
        "    #creating the arrays for the training and test year\n",
        "    train_year_type1=type1_crimes[type1_crimes['Year']==years[i-1]]\n",
        "    train_year_type2=type2_crimes[type2_crimes['Year']==years[i-1]]\n",
        "    test_year_type1=type1_crimes[type1_crimes['Year']==years[i]]\n",
        "    test_year_type2=type2_crimes[type2_crimes['Year']==years[i]]\n",
        "    #dropping the year column from both the training and test sets\n",
        "    train_year_type1=train_year_type1.drop(['Year'],axis=1)\n",
        "    train_year_type2=train_year_type2.drop(['Year'],axis=1)\n",
        "    test_year_type1=test_year_type1.drop(['Year'],axis=1)\n",
        "    test_year_type2=test_year_type2.drop(['Year'],axis=1)\n",
        "    #adding the numerical value for months in each set\n",
        "    train_year_type1['Month']=months\n",
        "    train_year_type2['Month']=months\n",
        "    test_year_type1['Month']=months\n",
        "    test_year_type2['Month']=months\n",
        "    #dropping the one hot encoded month columns\n",
        "    train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    #creating the X and Y arrays for the training and test sets\n",
        "    X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "    X_type2_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "    y_type1_train=train_year_type1['Type_I_crime_amount']\n",
        "    y_type2_train=train_year_type2['Type_II_crime_amount']\n",
        "    X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "    X_type2_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "    y_type1_test=test_year_type1['Type_I_crime_amount']\n",
        "    y_type2_test=test_year_type2['Type_II_crime_amount']\n",
        "    #creating two separate random forest regressor for each crime type\n",
        "    rf_type1=RandomForestRegressor(n_estimators=400,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',max_depth=18,bootstrap=False,random_state=0)\n",
        "    rf_type2=RandomForestRegressor(n_estimators=200,min_samples_split=10,min_samples_leaf=2,max_features='auto',max_depth=60,bootstrap=True,random_state=0)\n",
        "    rf_type1.fit(X_type1_train,y_type1_train)\n",
        "    rf_type2.fit(X_type2_train,y_type2_train)\n",
        "    #predicting the values for the test set\n",
        "    y_pred_type1=rf_type1.predict(X_type1_test)\n",
        "    y_pred_type2=rf_type2.predict(X_type2_test)\n",
        "    #calculating the r2 score for each crime type\n",
        "    r2_type1.append(r2_score(y_type1_test,y_pred_type1))\n",
        "    r2_type2.append(r2_score(y_type2_test,y_pred_type2))\n",
        "    #calculating the rmse for each crime type\n",
        "    rmse_type1.append(np.sqrt(mean_squared_error(y_type1_test,y_pred_type1)))\n",
        "    rmse_type2.append(np.sqrt(mean_squared_error(y_type2_test,y_pred_type2)))\n",
        "    #calculating the mae for each crime type\n",
        "    mae_type1.append(mean_absolute_error(y_type1_test,y_pred_type1,multioutput='raw_values'))\n",
        "    mae_type2.append(mean_absolute_error(y_type2_test,y_pred_type2,multioutput='raw_values'))\n",
        "    #calculating the median absolute error for each crime type\n",
        "    medae_type1.append(np.median(mae_type1[i-1]))\n",
        "    medae_type2.append(np.median(mae_type2[i-1]))\n",
        "\n",
        "#combining the array for the navigated years with the evaluation results for each metric    \n",
        "rmse_area2_type1=list(zip(year_sofar,rmse_type1))\n",
        "rmse_area2_type2=list(zip(year_sofar,rmse_type2))\n",
        "mae_area2_type1=list(zip(year_sofar,mae_type1))\n",
        "mae_area2_type2=list(zip(year_sofar,mae_type2))\n",
        "r2_area2_type2=list(zip(year_sofar,r2_type2))\n",
        "r2_area2_type1=list(zip(year_sofar,r2_type1))\n",
        "medae_area2_type1=list(zip(year_sofar,medae_type1))\n",
        "medae_area2_type2=list(zip(year_sofar,medae_type2))\n",
        "#creating dataframes for the evaluation results\n",
        "rmse_area2_type1_table=pd.DataFrame(rmse_area2_type1,columns=['Year','RMSE_Type_I'])\n",
        "rmse_area2_type2_table=pd.DataFrame(rmse_area2_type2,columns=['Year','RMSE_Type_II'])\n",
        "mae_area2_type1_table=pd.DataFrame(mae_area2_type1,columns=['Year','MAE_Type_I'])\n",
        "mae_area2_type2_table=pd.DataFrame(mae_area2_type2,columns=['Year','MAE_Type_II'])\n",
        "r2_area2_type1_table=pd.DataFrame(r2_area2_type1,columns=['Year','R2_Type_I'])\n",
        "r2_area2_type2_table=pd.DataFrame(r2_area2_type2,columns=['Year','R2_Type_II'])\n",
        "medae_area2_type1_table=pd.DataFrame(medae_area2_type1,columns=['Year','MedAE_Type_I'])\n",
        "medae_area2_type2_table=pd.DataFrame(medae_area2_type2,columns=['Year','MedAE_Type_II'])\n",
        "#outputting the data to csv files\n",
        "rmse_area2_type1_table.to_csv('rmse_area2_type1.csv')\n",
        "rmse_area2_type2_table.to_csv('rmse_area2_type2.csv')\n",
        "mae_area2_type1_table.to_csv('mae_area2_type1.csv')\n",
        "mae_area2_type2_table.to_csv('mae_area2_type2.csv')\n",
        "r2_area2_type1_table.to_csv('r2_area2_type1.csv')\n",
        "r2_area2_type2_table.to_csv('r2_area2_type2.csv')\n",
        "medae_area2_type1_table.to_csv('medae_area2_type1.csv')\n",
        "medae_area2_type2_table.to_csv('medae_area2_type2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8RVUCeibsRZ",
        "outputId": "f649762c-6d41-4e91-c7fb-d908ec2df09f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
              "                   n_jobs=-1,\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   random_state=35, verbose=2)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#creating the dataset which will be used for tuning\n",
        "type1_crimes=set3[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#Separating the data into test and training years\n",
        "train_year_type1=type1_crimes[type1_crimes['Year']==2012]\n",
        "test_year_type1=type1_crimes[type1_crimes['Year']==2013]\n",
        "#dropping the year column\n",
        "train_year_type1=train_year_type1.drop(['Year'],axis=1)\n",
        "test_year_type1=test_year_type1.drop(['Year'],axis=1)\n",
        "#adding the numerical values for months\n",
        "train_year_type1['Month']=months\n",
        "test_year_type1['Month']=months\n",
        "#dropping the one-hot encoded columns\n",
        "train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "#creating the X and Y variables for both training and test data\n",
        "X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "y_type1_train=train_year_type1['Type_I_crime_amount']\n",
        "X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "y_type1_test=test_year_type1['Type_I_crime_amount']\n",
        "#creating the random forest regressor and the hyperparameter tuning method\n",
        "rf=RandomForestRegressor()\n",
        "rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n",
        "rf_random.fit(X_type1_train,y_type1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEEUOFxObsRa",
        "outputId": "2aa20559-42f1-4c52-a554-af4be4a0e55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
              "                   n_jobs=-1,\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   random_state=35, verbose=2)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#creating the dataframe for the tuning of the hyperparameters\n",
        "type2_crimes=set3[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#Separating the data into test and training years\n",
        "train_year_type2=type2_crimes[type2_crimes['Year']==2012]\n",
        "test_year_type2=type2_crimes[type1_crimes['Year']==2013]\n",
        "#dropping the year column\n",
        "train_year_type2=train_year_type2.drop(['Year'],axis=1)\n",
        "test_year_type2=test_year_type2.drop(['Year'],axis=1)\n",
        "#adding the numerical values for months\n",
        "train_year_type2['Month']=months\n",
        "test_year_type2['Month']=months\n",
        "#dropping the one-hot encoded columns\n",
        "train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "#creating the X and Y variables for both training and test data\n",
        "X_type1_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "y_type1_train=train_year_type2['Type_II_crime_amount']\n",
        "X_type1_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "y_type1_test=test_year_type2['Type_II_crime_amount']\n",
        "#creating the random forest regressor and the hyperparameter tuning method\n",
        "rf=RandomForestRegressor()\n",
        "rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,random_state=35,n_jobs=-1)\n",
        "rf_random.fit(X_type1_train,y_type1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN-_3vzybsRa"
      },
      "outputs": [],
      "source": [
        "#printing the best parameters after tuning (must be run after each tune for each crime type)\n",
        "rf_random.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMM7tdb-bsRb"
      },
      "outputs": [],
      "source": [
        "#creating the dataframe for each crime type\n",
        "type1_crimes=set3[['Year','Type_I_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "type2_crimes=set3[['Year','Type_II_crime_amount','January','February','March','April','May','June','July','August','September','October','November','December']]\n",
        "#creating arrays where we will store the results\n",
        "year_sofar=[]\n",
        "r2_type1=[]\n",
        "r2_type2=[]\n",
        "rmse_type1=[]\n",
        "rmse_type2=[]\n",
        "mae_type1=[]\n",
        "mae_type2=[]\n",
        "year_sofar=[]\n",
        "medae_type1=[]\n",
        "medae_type2=[]\n",
        "#looping through the years\n",
        "for i in range(1,len(years)):\n",
        "    #addng the evaluated years to the array\n",
        "    year_sofar.append(years[i])\n",
        "    #creating the data for the training and test years\n",
        "    train_year_type1=type1_crimes[type1_crimes['Year']==years[i-1]]\n",
        "    train_year_type2=type2_crimes[type2_crimes['Year']==years[i-1]]\n",
        "    test_year_type1=type1_crimes[type1_crimes['Year']==years[i]]\n",
        "    test_year_type2=type2_crimes[type2_crimes['Year']==years[i]]\n",
        "    #dropping the year column\n",
        "    train_year_type1=train_year_type1.drop(['Year'],axis=1)\n",
        "    train_year_type2=train_year_type2.drop(['Year'],axis=1)\n",
        "    test_year_type1=test_year_type1.drop(['Year'],axis=1)\n",
        "    test_year_type2=test_year_type2.drop(['Year'],axis=1)\n",
        "    #adding the numerical values for months\n",
        "    train_year_type1['Month']=months\n",
        "    train_year_type2['Month']=months\n",
        "    test_year_type1['Month']=months\n",
        "    test_year_type2['Month']=months\n",
        "    #dropping the one-hot encoded columns\n",
        "    train_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    train_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    test_year_type1.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    test_year_type2.drop(['January','February','March','April','May','June','July','August','September','October','November','December'],axis=1,inplace=True)\n",
        "    #creating the X and Y variables for both training and test data\n",
        "    X_type1_train=train_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "    X_type2_train=train_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "    y_type1_train=train_year_type1['Type_I_crime_amount']\n",
        "    y_type2_train=train_year_type2['Type_II_crime_amount']\n",
        "    X_type1_test=test_year_type1.drop(['Type_I_crime_amount'],axis=1)\n",
        "    X_type2_test=test_year_type2.drop(['Type_II_crime_amount'],axis=1)\n",
        "    y_type1_test=test_year_type1['Type_I_crime_amount']\n",
        "    y_type2_test=test_year_type2['Type_II_crime_amount']\n",
        "    #creating the random forest regressors for each crime tyoe\n",
        "    rf_type1=RandomForestRegressor(n_estimators=600,min_samples_split=10,min_samples_leaf=2,max_features='sqrt',max_depth=None,bootstrap=True,random_state=0)\n",
        "    rf_type2=RandomForestRegressor(n_estimators=800,min_samples_split=5,min_samples_leaf=4,max_features='auto',max_depth=110,bootstrap=True,random_state=0)\n",
        "    rf_type1.fit(X_type1_train,y_type1_train)\n",
        "    rf_type2.fit(X_type2_train,y_type2_train)\n",
        "    #predicting the results for each crime type\n",
        "    y_pred_type1=rf_type1.predict(X_type1_test)\n",
        "    y_pred_type2=rf_type2.predict(X_type2_test)\n",
        "    #calculating the r2 score for each crime type\n",
        "    r2_type1.append(r2_score(y_type1_test,y_pred_type1))\n",
        "    r2_type2.append(r2_score(y_type2_test,y_pred_type2))\n",
        "    #calculating the rmse for each crime type\n",
        "    rmse_type1.append(np.sqrt(mean_squared_error(y_type1_test,y_pred_type1)))\n",
        "    rmse_type2.append(np.sqrt(mean_squared_error(y_type2_test,y_pred_type2)))\n",
        "    #calculating the mae for each crime type\n",
        "    mae_type1.append(mean_absolute_error(y_type1_test,y_pred_type1,multioutput='raw_values'))\n",
        "    mae_type2.append(mean_absolute_error(y_type2_test,y_pred_type2,multioutput='raw_values'))\n",
        "    #calculating the median absolute error for each crime type\n",
        "    medae_type1.append(np.median(mae_type1))\n",
        "    medae_type2.append(np.median(mae_type2))\n",
        "#combining the data for the years for each evaluation metric\n",
        "rmse_area3_type1=list(zip(year_sofar,rmse_type1))\n",
        "rmse_area3_type2=list(zip(year_sofar,rmse_type2))\n",
        "mae_area3_type1=list(zip(year_sofar,mae_type1))\n",
        "mae_area3_type2=list(zip(year_sofar,mae_type2))\n",
        "r2_area3_type2=list(zip(year_sofar,r2_type2))\n",
        "r2_area3_type1=list(zip(year_sofar,r2_type1))\n",
        "medae_area3_type1=list(zip(year_sofar,medae_type1))\n",
        "medae_area3_type2=list(zip(year_sofar,medae_type2))\n",
        "#creating the dataframes for the results\n",
        "rmse_area3_type1_table=pd.DataFrame(rmse_area3_type1,columns=['Year','RMSE_Type_I'])\n",
        "rmse_area3_type2_table=pd.DataFrame(rmse_area3_type2,columns=['Year','RMSE_Type_II'])\n",
        "mae_area3_type1_table=pd.DataFrame(mae_area3_type1,columns=['Year','MAE_Type_I'])\n",
        "mae_area3_type2_table=pd.DataFrame(mae_area3_type2,columns=['Year','MAE_Type_II'])\n",
        "r2_area3_type1_table=pd.DataFrame(r2_area3_type1,columns=['Year','R2_Type_I'])\n",
        "r2_area3_type2_table=pd.DataFrame(r2_area3_type2,columns=['Year','R2_Type_II'])\n",
        "medae_area3_type1_table=pd.DataFrame(medae_area3_type1,columns=['Year','MedAE_Type_I'])\n",
        "medae_area3_type2_table=pd.DataFrame(medae_area3_type2,columns=['Year','MedAE_Type_II'])\n",
        "#outputting the dataframes to csv files\n",
        "rmse_area3_type1_table.to_csv('rmse_area3_type1.csv')\n",
        "rmse_area3_type2_table.to_csv('rmse_area3_type2.csv')\n",
        "mae_area3_type1_table.to_csv('mae_area3_type1.csv')\n",
        "mae_area3_type2_table.to_csv('mae_area3_type2.csv')\n",
        "r2_area3_type1_table.to_csv('r2_area3_type1.csv')\n",
        "r2_area3_type2_table.to_csv('r2_area3_type2.csv')\n",
        "medae_area3_type1_table.to_csv('medae_area3_type1.csv')\n",
        "medae_area3_type2_table.to_csv('medae_area3_type2.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Baseline Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}